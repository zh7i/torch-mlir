func.func @main(%arg0: !torch.vtensor<[10,32],si64>) -> !torch.vtensor<[10,32,1000],f32> {
  %true = torch.constant.bool true
  %0 = torch.vtensor.literal(dense_resource<torch_tensor_1000_torch.float32> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
  %1 = torch.vtensor.literal(dense_resource<torch_tensor_1000_256_torch.float32_1> : tensor<1000x256xf32>) : !torch.vtensor<[1000,256],f32>
  %2 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_11> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %3 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_10> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %4 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_9> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %5 = torch.vtensor.literal(dense_resource<torch_tensor_256_2048_torch.float32_1> : tensor<256x2048xf32>) : !torch.vtensor<[256,2048],f32>
  %6 = torch.vtensor.literal(dense_resource<torch_tensor_2048_torch.float32_1> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
  %7 = torch.vtensor.literal(dense_resource<torch_tensor_2048_256_torch.float32_1> : tensor<2048x256xf32>) : !torch.vtensor<[2048,256],f32>
  %8 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_8> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %9 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_7> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %10 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_6> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %11 = torch.vtensor.literal(dense_resource<torch_tensor_256_256_torch.float32_1> : tensor<256x256xf32>) : !torch.vtensor<[256,256],f32>
  %12 = torch.vtensor.literal(dense_resource<torch_tensor_768_torch.float32_1> : tensor<768xf32>) : !torch.vtensor<[768],f32>
  %13 = torch.vtensor.literal(dense_resource<torch_tensor_768_256_torch.float32_1> : tensor<768x256xf32>) : !torch.vtensor<[768,256],f32>
  %14 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_5> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %15 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_4> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %16 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_3> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %17 = torch.vtensor.literal(dense_resource<torch_tensor_256_2048_torch.float32> : tensor<256x2048xf32>) : !torch.vtensor<[256,2048],f32>
  %18 = torch.vtensor.literal(dense_resource<torch_tensor_2048_torch.float32> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
  %19 = torch.vtensor.literal(dense_resource<torch_tensor_2048_256_torch.float32> : tensor<2048x256xf32>) : !torch.vtensor<[2048,256],f32>
  %float1.000000e-05 = torch.constant.float 1.000000e-05
  %20 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_2> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %21 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_1> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %22 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %23 = torch.vtensor.literal(dense_resource<torch_tensor_256_256_torch.float32> : tensor<256x256xf32>) : !torch.vtensor<[256,256],f32>
  %int320 = torch.constant.int 320
  %float0.000000e00 = torch.constant.float 0.000000e+00
  %none = torch.constant.none
  %int4 = torch.constant.int 4
  %int64 = torch.constant.int 64
  %int128 = torch.constant.int 128
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int-2 = torch.constant.int -2
  %int0 = torch.constant.int 0
  %int256 = torch.constant.int 256
  %int3 = torch.constant.int 3
  %int32 = torch.constant.int 32
  %int10 = torch.constant.int 10
  %24 = torch.vtensor.literal(dense_resource<torch_tensor_768_torch.float32> : tensor<768xf32>) : !torch.vtensor<[768],f32>
  %25 = torch.vtensor.literal(dense_resource<torch_tensor_768_256_torch.float32> : tensor<768x256xf32>) : !torch.vtensor<[768,256],f32>
  %26 = torch.vtensor.literal(dense_resource<torch_tensor_1000_256_torch.float32> : tensor<1000x256xf32>) : !torch.vtensor<[1000,256],f32>
  %int-1 = torch.constant.int -1
  %false = torch.constant.bool false
  %27 = torch.aten.embedding %26, %arg0, %int-1, %false, %false : !torch.vtensor<[1000,256],f32>, !torch.vtensor<[10,32],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[10,32,256],f32>
  %28 = torch.aten.transpose.int %25, %int0, %int1 : !torch.vtensor<[768,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,768],f32>
  %29 = torch.aten.matmul %27, %28 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,768],f32> -> !torch.vtensor<[10,32,768],f32>
  %30 = torch.aten.add.Tensor %29, %24, %int1 : !torch.vtensor<[10,32,768],f32>, !torch.vtensor<[768],f32>, !torch.int -> !torch.vtensor<[10,32,768],f32>
  %31 = torch.prim.ListConstruct %int10, %int32, %int3, %int256 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %32 = torch.aten.view %30, %31 : !torch.vtensor<[10,32,768],f32>, !torch.list<int> -> !torch.vtensor<[10,32,3,256],f32>
  %33 = torch.aten.unsqueeze %32, %int0 : !torch.vtensor<[10,32,3,256],f32>, !torch.int -> !torch.vtensor<[1,10,32,3,256],f32>
  %34 = torch.aten.transpose.int %33, %int0, %int-2 : !torch.vtensor<[1,10,32,3,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[3,10,32,1,256],f32>
  %35 = torch.aten.squeeze.dim %34, %int-2 : !torch.vtensor<[3,10,32,1,256],f32>, !torch.int -> !torch.vtensor<[3,10,32,256],f32>
  %36 = torch.aten.slice.Tensor %35, %int0, %int0, %int1, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %37 = torch.aten.squeeze.dim %36, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %38 = torch.aten.slice.Tensor %35, %int0, %int1, %int2, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %39 = torch.aten.squeeze.dim %38, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %40 = torch.aten.slice.Tensor %35, %int0, %int2, %int3, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %41 = torch.aten.squeeze.dim %40, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %42 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %43 = torch.aten.view %37, %42 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %44 = torch.aten.transpose.int %43, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %45 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %46 = torch.aten.view %39, %45 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %47 = torch.aten.transpose.int %46, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %48 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %49 = torch.aten.view %41, %48 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %50 = torch.aten.transpose.int %49, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %51 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %52 = torch.aten.view %44, %51 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %53 = torch_c.to_builtin_tensor %52 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %54 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %55 = torch.aten.view %47, %54 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %56 = torch_c.to_builtin_tensor %55 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %57 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %58 = torch.aten.view %50, %57 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %59 = torch_c.to_builtin_tensor %58 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %collapsed = tensor.collapse_shape %53 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %collapsed_0 = tensor.collapse_shape %56 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %collapsed_1 = tensor.collapse_shape %59 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c10 = arith.constant 10 : index
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0_2 = arith.constant 0 : index
  %c128_3 = arith.constant 128 : index
  %c1_4 = arith.constant 1 : index
  %c10_5 = arith.constant 10 : index
  %c2_6 = arith.constant 2 : index
  %c64_7 = arith.constant 64 : index
  %60 = tensor.empty() : tensor<128x10x64xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %61 = linalg.fill ins(%cst : f32) outs(%60 : tensor<128x10x64xf32>) -> tensor<128x10x64xf32>
  %62 = tm_tensor.attention ins(%collapsed, %collapsed_0, %collapsed_1 : tensor<128x10x64xf32>, tensor<128x10x64xf32>, tensor<128x10x64xf32>) outs(%61 : tensor<128x10x64xf32>) -> tensor<128x10x64xf32>
  %expanded = tensor.expand_shape %62 [[0, 1], [2], [3]] output_shape [32, 4, 10, 64] : tensor<128x10x64xf32> into tensor<32x4x10x64xf32>
  %63 = torch_c.from_builtin_tensor %expanded : tensor<32x4x10x64xf32> -> !torch.vtensor<[32,4,10,64],f32>
  %64 = torch.prim.ListConstruct %int2, %int0, %int1, %int3 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %65 = torch.aten.permute %63, %64 : !torch.vtensor<[32,4,10,64],f32>, !torch.list<int> -> !torch.vtensor<[10,32,4,64],f32>
  %66 = torch.prim.ListConstruct %int320, %int256 : (!torch.int, !torch.int) -> !torch.list<int>
  %67 = torch.aten.view %65, %66 : !torch.vtensor<[10,32,4,64],f32>, !torch.list<int> -> !torch.vtensor<[320,256],f32>
  %68 = torch.aten.transpose.int %23, %int0, %int1 : !torch.vtensor<[256,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,256],f32>
  %69 = torch.aten.mm %67, %68 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256,256],f32> -> !torch.vtensor<[320,256],f32>
  %70 = torch.aten.add.Tensor %69, %22, %int1 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[320,256],f32>
  %71 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %72 = torch.aten.view %70, %71 : !torch.vtensor<[320,256],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %73 = torch.aten.add.Tensor %27, %72, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %74 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %75 = torch.aten.sum.dim_IntList %73, %74, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %76 = torch.aten.div.Scalar %75, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %77 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %78 = torch.aten.broadcast_to %76, %77 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %79 = torch.aten.sub.Tensor %73, %78, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %80 = torch.aten.mul.Tensor %79, %79 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %81 = torch.aten.sum.dim_IntList %80, %74, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %82 = torch.aten.div.Scalar %81, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %83 = torch.aten.add.Scalar %82, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %84 = torch.aten.rsqrt %83 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %85 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %86 = torch.aten.broadcast_to %84, %85 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %87 = torch.aten.mul.Tensor %79, %86 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %88 = torch.aten.mul.Tensor %87, %21 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %89 = torch.aten.add.Tensor %88, %20, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %90 = torch.aten.transpose.int %19, %int0, %int1 : !torch.vtensor<[2048,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,2048],f32>
  %91 = torch.aten.matmul %89, %90 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %92 = torch.aten.add.Tensor %91, %18, %int1 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048],f32>, !torch.int -> !torch.vtensor<[10,32,2048],f32>
  %93 = torch.aten.relu %92 : !torch.vtensor<[10,32,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %94 = torch.aten.transpose.int %17, %int0, %int1 : !torch.vtensor<[256,2048],f32>, !torch.int, !torch.int -> !torch.vtensor<[2048,256],f32>
  %95 = torch.aten.matmul %93, %94 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %96 = torch.aten.add.Tensor %95, %16, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %97 = torch.aten.add.Tensor %89, %96, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %98 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %99 = torch.aten.sum.dim_IntList %97, %98, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %100 = torch.aten.div.Scalar %99, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %101 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %102 = torch.aten.broadcast_to %100, %101 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %103 = torch.aten.sub.Tensor %97, %102, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %104 = torch.aten.mul.Tensor %103, %103 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %105 = torch.aten.sum.dim_IntList %104, %98, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %106 = torch.aten.div.Scalar %105, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %107 = torch.aten.add.Scalar %106, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %108 = torch.aten.rsqrt %107 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %109 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %110 = torch.aten.broadcast_to %108, %109 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %111 = torch.aten.mul.Tensor %103, %110 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %112 = torch.aten.mul.Tensor %111, %15 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %113 = torch.aten.add.Tensor %112, %14, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %114 = torch.aten.transpose.int %13, %int0, %int1 : !torch.vtensor<[768,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,768],f32>
  %115 = torch.aten.matmul %113, %114 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,768],f32> -> !torch.vtensor<[10,32,768],f32>
  %116 = torch.aten.add.Tensor %115, %12, %int1 : !torch.vtensor<[10,32,768],f32>, !torch.vtensor<[768],f32>, !torch.int -> !torch.vtensor<[10,32,768],f32>
  %117 = torch.prim.ListConstruct %int10, %int32, %int3, %int256 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %118 = torch.aten.view %116, %117 : !torch.vtensor<[10,32,768],f32>, !torch.list<int> -> !torch.vtensor<[10,32,3,256],f32>
  %119 = torch.aten.unsqueeze %118, %int0 : !torch.vtensor<[10,32,3,256],f32>, !torch.int -> !torch.vtensor<[1,10,32,3,256],f32>
  %120 = torch.aten.transpose.int %119, %int0, %int-2 : !torch.vtensor<[1,10,32,3,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[3,10,32,1,256],f32>
  %121 = torch.aten.squeeze.dim %120, %int-2 : !torch.vtensor<[3,10,32,1,256],f32>, !torch.int -> !torch.vtensor<[3,10,32,256],f32>
  %122 = torch.aten.slice.Tensor %121, %int0, %int0, %int1, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %123 = torch.aten.squeeze.dim %122, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %124 = torch.aten.slice.Tensor %121, %int0, %int1, %int2, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %125 = torch.aten.squeeze.dim %124, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %126 = torch.aten.slice.Tensor %121, %int0, %int2, %int3, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %127 = torch.aten.squeeze.dim %126, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %128 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %129 = torch.aten.view %123, %128 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %130 = torch.aten.transpose.int %129, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %131 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %132 = torch.aten.view %125, %131 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %133 = torch.aten.transpose.int %132, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %134 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %135 = torch.aten.view %127, %134 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %136 = torch.aten.transpose.int %135, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %137 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %138 = torch.aten.view %130, %137 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %139 = torch_c.to_builtin_tensor %138 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %140 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %141 = torch.aten.view %133, %140 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %142 = torch_c.to_builtin_tensor %141 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %143 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %144 = torch.aten.view %136, %143 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %145 = torch_c.to_builtin_tensor %144 : !torch.vtensor<[32,4,10,64],f32> -> tensor<32x4x10x64xf32>
  %collapsed_8 = tensor.collapse_shape %139 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %collapsed_9 = tensor.collapse_shape %142 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %collapsed_10 = tensor.collapse_shape %145 [[0, 1], [2], [3]] : tensor<32x4x10x64xf32> into tensor<128x10x64xf32>
  %c0_11 = arith.constant 0 : index
  %c128_12 = arith.constant 128 : index
  %c1_13 = arith.constant 1 : index
  %c10_14 = arith.constant 10 : index
  %c2_15 = arith.constant 2 : index
  %c64_16 = arith.constant 64 : index
  %c0_17 = arith.constant 0 : index
  %c128_18 = arith.constant 128 : index
  %c1_19 = arith.constant 1 : index
  %c10_20 = arith.constant 10 : index
  %c2_21 = arith.constant 2 : index
  %c64_22 = arith.constant 64 : index
  %146 = tensor.empty() : tensor<128x10x64xf32>
  %cst_23 = arith.constant 0.000000e+00 : f32
  %147 = linalg.fill ins(%cst_23 : f32) outs(%146 : tensor<128x10x64xf32>) -> tensor<128x10x64xf32>
  %148 = tm_tensor.attention ins(%collapsed_8, %collapsed_9, %collapsed_10 : tensor<128x10x64xf32>, tensor<128x10x64xf32>, tensor<128x10x64xf32>) outs(%147 : tensor<128x10x64xf32>) -> tensor<128x10x64xf32>
  %expanded_24 = tensor.expand_shape %148 [[0, 1], [2], [3]] output_shape [32, 4, 10, 64] : tensor<128x10x64xf32> into tensor<32x4x10x64xf32>
  %149 = torch_c.from_builtin_tensor %expanded_24 : tensor<32x4x10x64xf32> -> !torch.vtensor<[32,4,10,64],f32>
  %150 = torch.prim.ListConstruct %int2, %int0, %int1, %int3 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %151 = torch.aten.permute %149, %150 : !torch.vtensor<[32,4,10,64],f32>, !torch.list<int> -> !torch.vtensor<[10,32,4,64],f32>
  %152 = torch.prim.ListConstruct %int320, %int256 : (!torch.int, !torch.int) -> !torch.list<int>
  %153 = torch.aten.view %151, %152 : !torch.vtensor<[10,32,4,64],f32>, !torch.list<int> -> !torch.vtensor<[320,256],f32>
  %154 = torch.aten.transpose.int %11, %int0, %int1 : !torch.vtensor<[256,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,256],f32>
  %155 = torch.aten.mm %153, %154 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256,256],f32> -> !torch.vtensor<[320,256],f32>
  %156 = torch.aten.add.Tensor %155, %10, %int1 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[320,256],f32>
  %157 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %158 = torch.aten.view %156, %157 : !torch.vtensor<[320,256],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %159 = torch.aten.add.Tensor %113, %158, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %160 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %161 = torch.aten.sum.dim_IntList %159, %160, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %162 = torch.aten.div.Scalar %161, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %163 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %164 = torch.aten.broadcast_to %162, %163 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %165 = torch.aten.sub.Tensor %159, %164, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %166 = torch.aten.mul.Tensor %165, %165 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %167 = torch.aten.sum.dim_IntList %166, %160, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %168 = torch.aten.div.Scalar %167, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %169 = torch.aten.add.Scalar %168, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %170 = torch.aten.rsqrt %169 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %171 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %172 = torch.aten.broadcast_to %170, %171 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %173 = torch.aten.mul.Tensor %165, %172 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %174 = torch.aten.mul.Tensor %173, %9 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %175 = torch.aten.add.Tensor %174, %8, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %176 = torch.aten.transpose.int %7, %int0, %int1 : !torch.vtensor<[2048,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,2048],f32>
  %177 = torch.aten.matmul %175, %176 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %178 = torch.aten.add.Tensor %177, %6, %int1 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048],f32>, !torch.int -> !torch.vtensor<[10,32,2048],f32>
  %179 = torch.aten.relu %178 : !torch.vtensor<[10,32,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %180 = torch.aten.transpose.int %5, %int0, %int1 : !torch.vtensor<[256,2048],f32>, !torch.int, !torch.int -> !torch.vtensor<[2048,256],f32>
  %181 = torch.aten.matmul %179, %180 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %182 = torch.aten.add.Tensor %181, %4, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %183 = torch.aten.add.Tensor %175, %182, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %184 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %185 = torch.aten.sum.dim_IntList %183, %184, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %186 = torch.aten.div.Scalar %185, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %187 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %188 = torch.aten.broadcast_to %186, %187 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %189 = torch.aten.sub.Tensor %183, %188, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %190 = torch.aten.mul.Tensor %189, %189 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %191 = torch.aten.sum.dim_IntList %190, %184, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %192 = torch.aten.div.Scalar %191, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %193 = torch.aten.add.Scalar %192, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %194 = torch.aten.rsqrt %193 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %195 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %196 = torch.aten.broadcast_to %194, %195 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %197 = torch.aten.mul.Tensor %189, %196 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %198 = torch.aten.mul.Tensor %197, %3 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %199 = torch.aten.add.Tensor %198, %2, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %200 = torch.aten.transpose.int %1, %int0, %int1 : !torch.vtensor<[1000,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,1000],f32>
  %201 = torch.aten.matmul %199, %200 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,1000],f32> -> !torch.vtensor<[10,32,1000],f32>
  %202 = torch.aten.add.Tensor %201, %0, %int1 : !torch.vtensor<[10,32,1000],f32>, !torch.vtensor<[1000],f32>, !torch.int -> !torch.vtensor<[10,32,1000],f32>
  return %202 : !torch.vtensor<[10,32,1000],f32>
}