func.func @main(%arg0: !torch.vtensor<[10,32],si64>) -> !torch.vtensor<[10,32,1000],f32> {
  %true = torch.constant.bool true
  %0 = torch.vtensor.literal(dense_resource<torch_tensor_1000_torch.float32> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
  %1 = torch.vtensor.literal(dense_resource<torch_tensor_1000_256_torch.float32_1> : tensor<1000x256xf32>) : !torch.vtensor<[1000,256],f32>
  %2 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_11> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %3 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_10> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %4 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_9> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %5 = torch.vtensor.literal(dense_resource<torch_tensor_256_2048_torch.float32_1> : tensor<256x2048xf32>) : !torch.vtensor<[256,2048],f32>
  %6 = torch.vtensor.literal(dense_resource<torch_tensor_2048_torch.float32_1> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
  %7 = torch.vtensor.literal(dense_resource<torch_tensor_2048_256_torch.float32_1> : tensor<2048x256xf32>) : !torch.vtensor<[2048,256],f32>
  %8 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_8> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %9 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_7> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %10 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_6> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %11 = torch.vtensor.literal(dense_resource<torch_tensor_256_256_torch.float32_1> : tensor<256x256xf32>) : !torch.vtensor<[256,256],f32>
  %12 = torch.vtensor.literal(dense_resource<torch_tensor_768_torch.float32_1> : tensor<768xf32>) : !torch.vtensor<[768],f32>
  %13 = torch.vtensor.literal(dense_resource<torch_tensor_768_256_torch.float32_1> : tensor<768x256xf32>) : !torch.vtensor<[768,256],f32>
  %14 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_5> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %15 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_4> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %16 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_3> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %17 = torch.vtensor.literal(dense_resource<torch_tensor_256_2048_torch.float32> : tensor<256x2048xf32>) : !torch.vtensor<[256,2048],f32>
  %18 = torch.vtensor.literal(dense_resource<torch_tensor_2048_torch.float32> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
  %19 = torch.vtensor.literal(dense_resource<torch_tensor_2048_256_torch.float32> : tensor<2048x256xf32>) : !torch.vtensor<[2048,256],f32>
  %float1.000000e-05 = torch.constant.float 1.000000e-05
  %20 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_2> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %21 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32_1> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %22 = torch.vtensor.literal(dense_resource<torch_tensor_256_torch.float32> : tensor<256xf32>) : !torch.vtensor<[256],f32>
  %23 = torch.vtensor.literal(dense_resource<torch_tensor_256_256_torch.float32> : tensor<256x256xf32>) : !torch.vtensor<[256,256],f32>
  %int320 = torch.constant.int 320
  %float0.000000e00 = torch.constant.float 0.000000e+00
  %none = torch.constant.none
  %int4 = torch.constant.int 4
  %int64 = torch.constant.int 64
  %int128 = torch.constant.int 128
  %int2 = torch.constant.int 2
  %int1 = torch.constant.int 1
  %int-2 = torch.constant.int -2
  %int0 = torch.constant.int 0
  %int256 = torch.constant.int 256
  %int3 = torch.constant.int 3
  %int32 = torch.constant.int 32
  %int10 = torch.constant.int 10
  %24 = torch.vtensor.literal(dense_resource<torch_tensor_768_torch.float32> : tensor<768xf32>) : !torch.vtensor<[768],f32>
  %25 = torch.vtensor.literal(dense_resource<torch_tensor_768_256_torch.float32> : tensor<768x256xf32>) : !torch.vtensor<[768,256],f32>
  %26 = torch.vtensor.literal(dense_resource<torch_tensor_1000_256_torch.float32> : tensor<1000x256xf32>) : !torch.vtensor<[1000,256],f32>
  %int-1 = torch.constant.int -1
  %false = torch.constant.bool false
  %27 = torch.aten.embedding %26, %arg0, %int-1, %false, %false : !torch.vtensor<[1000,256],f32>, !torch.vtensor<[10,32],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[10,32,256],f32>
  %28 = torch.aten.transpose.int %25, %int0, %int1 : !torch.vtensor<[768,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,768],f32>
  %29 = torch.aten.matmul %27, %28 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,768],f32> -> !torch.vtensor<[10,32,768],f32>
  %30 = torch.aten.add.Tensor %29, %24, %int1 : !torch.vtensor<[10,32,768],f32>, !torch.vtensor<[768],f32>, !torch.int -> !torch.vtensor<[10,32,768],f32>
  %31 = torch.prim.ListConstruct %int10, %int32, %int3, %int256 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %32 = torch.aten.view %30, %31 : !torch.vtensor<[10,32,768],f32>, !torch.list<int> -> !torch.vtensor<[10,32,3,256],f32>
  %33 = torch.aten.unsqueeze %32, %int0 : !torch.vtensor<[10,32,3,256],f32>, !torch.int -> !torch.vtensor<[1,10,32,3,256],f32>
  %34 = torch.aten.transpose.int %33, %int0, %int-2 : !torch.vtensor<[1,10,32,3,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[3,10,32,1,256],f32>
  %35 = torch.aten.squeeze.dim %34, %int-2 : !torch.vtensor<[3,10,32,1,256],f32>, !torch.int -> !torch.vtensor<[3,10,32,256],f32>
  %36 = torch.aten.slice.Tensor %35, %int0, %int0, %int1, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %37 = torch.aten.squeeze.dim %36, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %38 = torch.aten.slice.Tensor %35, %int0, %int1, %int2, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %39 = torch.aten.squeeze.dim %38, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %40 = torch.aten.slice.Tensor %35, %int0, %int2, %int3, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %41 = torch.aten.squeeze.dim %40, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %42 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %43 = torch.aten.view %37, %42 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %44 = torch.aten.transpose.int %43, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %45 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %46 = torch.aten.view %39, %45 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %47 = torch.aten.transpose.int %46, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %48 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %49 = torch.aten.view %41, %48 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %50 = torch.aten.transpose.int %49, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %51 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %52 = torch.aten.view %44, %51 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %53 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %54 = torch.aten.view %47, %53 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %55 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %56 = torch.aten.view %50, %55 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %57 = torch.aten.scaled_dot_product_attention %52, %54, %56, %none, %float0.000000e00, %false, %none, %false : !torch.vtensor<[32,4,10,64],f32>, !torch.vtensor<[32,4,10,64],f32>, !torch.vtensor<[32,4,10,64],f32>, !torch.none, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[32,4,10,64],f32>
  %58 = torch.prim.ListConstruct %int2, %int0, %int1, %int3 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %59 = torch.aten.permute %57, %58 : !torch.vtensor<[32,4,10,64],f32>, !torch.list<int> -> !torch.vtensor<[10,32,4,64],f32>
  %60 = torch.prim.ListConstruct %int320, %int256 : (!torch.int, !torch.int) -> !torch.list<int>
  %61 = torch.aten.view %59, %60 : !torch.vtensor<[10,32,4,64],f32>, !torch.list<int> -> !torch.vtensor<[320,256],f32>
  %62 = torch.aten.transpose.int %23, %int0, %int1 : !torch.vtensor<[256,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,256],f32>
  %63 = torch.aten.mm %61, %62 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256,256],f32> -> !torch.vtensor<[320,256],f32>
  %64 = torch.aten.add.Tensor %63, %22, %int1 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[320,256],f32>
  %65 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %66 = torch.aten.view %64, %65 : !torch.vtensor<[320,256],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %67 = torch.aten.add.Tensor %27, %66, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %68 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %69 = torch.aten.sum.dim_IntList %67, %68, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %70 = torch.aten.div.Scalar %69, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %71 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %72 = torch.aten.broadcast_to %70, %71 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %73 = torch.aten.sub.Tensor %67, %72, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %74 = torch.aten.mul.Tensor %73, %73 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %75 = torch.aten.sum.dim_IntList %74, %68, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %76 = torch.aten.div.Scalar %75, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %77 = torch.aten.add.Scalar %76, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %78 = torch.aten.rsqrt %77 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %79 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %80 = torch.aten.broadcast_to %78, %79 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %81 = torch.aten.mul.Tensor %73, %80 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %82 = torch.aten.mul.Tensor %81, %21 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %83 = torch.aten.add.Tensor %82, %20, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %84 = torch.aten.transpose.int %19, %int0, %int1 : !torch.vtensor<[2048,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,2048],f32>
  %85 = torch.aten.matmul %83, %84 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %86 = torch.aten.add.Tensor %85, %18, %int1 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048],f32>, !torch.int -> !torch.vtensor<[10,32,2048],f32>
  %87 = torch.aten.relu %86 : !torch.vtensor<[10,32,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %88 = torch.aten.transpose.int %17, %int0, %int1 : !torch.vtensor<[256,2048],f32>, !torch.int, !torch.int -> !torch.vtensor<[2048,256],f32>
  %89 = torch.aten.matmul %87, %88 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %90 = torch.aten.add.Tensor %89, %16, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %91 = torch.aten.add.Tensor %83, %90, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %92 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %93 = torch.aten.sum.dim_IntList %91, %92, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %94 = torch.aten.div.Scalar %93, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %95 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %96 = torch.aten.broadcast_to %94, %95 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %97 = torch.aten.sub.Tensor %91, %96, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %98 = torch.aten.mul.Tensor %97, %97 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %99 = torch.aten.sum.dim_IntList %98, %92, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %100 = torch.aten.div.Scalar %99, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %101 = torch.aten.add.Scalar %100, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %102 = torch.aten.rsqrt %101 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %103 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %104 = torch.aten.broadcast_to %102, %103 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %105 = torch.aten.mul.Tensor %97, %104 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %106 = torch.aten.mul.Tensor %105, %15 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %107 = torch.aten.add.Tensor %106, %14, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %108 = torch.aten.transpose.int %13, %int0, %int1 : !torch.vtensor<[768,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,768],f32>
  %109 = torch.aten.matmul %107, %108 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,768],f32> -> !torch.vtensor<[10,32,768],f32>
  %110 = torch.aten.add.Tensor %109, %12, %int1 : !torch.vtensor<[10,32,768],f32>, !torch.vtensor<[768],f32>, !torch.int -> !torch.vtensor<[10,32,768],f32>
  %111 = torch.prim.ListConstruct %int10, %int32, %int3, %int256 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %112 = torch.aten.view %110, %111 : !torch.vtensor<[10,32,768],f32>, !torch.list<int> -> !torch.vtensor<[10,32,3,256],f32>
  %113 = torch.aten.unsqueeze %112, %int0 : !torch.vtensor<[10,32,3,256],f32>, !torch.int -> !torch.vtensor<[1,10,32,3,256],f32>
  %114 = torch.aten.transpose.int %113, %int0, %int-2 : !torch.vtensor<[1,10,32,3,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[3,10,32,1,256],f32>
  %115 = torch.aten.squeeze.dim %114, %int-2 : !torch.vtensor<[3,10,32,1,256],f32>, !torch.int -> !torch.vtensor<[3,10,32,256],f32>
  %116 = torch.aten.slice.Tensor %115, %int0, %int0, %int1, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %117 = torch.aten.squeeze.dim %116, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %118 = torch.aten.slice.Tensor %115, %int0, %int1, %int2, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %119 = torch.aten.squeeze.dim %118, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %120 = torch.aten.slice.Tensor %115, %int0, %int2, %int3, %int1 : !torch.vtensor<[3,10,32,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,10,32,256],f32>
  %121 = torch.aten.squeeze.dim %120, %int0 : !torch.vtensor<[1,10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %122 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %123 = torch.aten.view %117, %122 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %124 = torch.aten.transpose.int %123, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %125 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %126 = torch.aten.view %119, %125 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %127 = torch.aten.transpose.int %126, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %128 = torch.prim.ListConstruct %int10, %int128, %int64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %129 = torch.aten.view %121, %128 : !torch.vtensor<[10,32,256],f32>, !torch.list<int> -> !torch.vtensor<[10,128,64],f32>
  %130 = torch.aten.transpose.int %129, %int0, %int1 : !torch.vtensor<[10,128,64],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,10,64],f32>
  %131 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %132 = torch.aten.view %124, %131 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %133 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %134 = torch.aten.view %127, %133 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %135 = torch.prim.ListConstruct %int32, %int4, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %136 = torch.aten.view %130, %135 : !torch.vtensor<[128,10,64],f32>, !torch.list<int> -> !torch.vtensor<[32,4,10,64],f32>
  %137 = torch.aten.scaled_dot_product_attention %132, %134, %136, %none, %float0.000000e00, %false, %none, %false : !torch.vtensor<[32,4,10,64],f32>, !torch.vtensor<[32,4,10,64],f32>, !torch.vtensor<[32,4,10,64],f32>, !torch.none, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[32,4,10,64],f32>
  %138 = torch.prim.ListConstruct %int2, %int0, %int1, %int3 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %139 = torch.aten.permute %137, %138 : !torch.vtensor<[32,4,10,64],f32>, !torch.list<int> -> !torch.vtensor<[10,32,4,64],f32>
  %140 = torch.prim.ListConstruct %int320, %int256 : (!torch.int, !torch.int) -> !torch.list<int>
  %141 = torch.aten.view %139, %140 : !torch.vtensor<[10,32,4,64],f32>, !torch.list<int> -> !torch.vtensor<[320,256],f32>
  %142 = torch.aten.transpose.int %11, %int0, %int1 : !torch.vtensor<[256,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,256],f32>
  %143 = torch.aten.mm %141, %142 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256,256],f32> -> !torch.vtensor<[320,256],f32>
  %144 = torch.aten.add.Tensor %143, %10, %int1 : !torch.vtensor<[320,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[320,256],f32>
  %145 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %146 = torch.aten.view %144, %145 : !torch.vtensor<[320,256],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %147 = torch.aten.add.Tensor %107, %146, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %148 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %149 = torch.aten.sum.dim_IntList %147, %148, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %150 = torch.aten.div.Scalar %149, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %151 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %152 = torch.aten.broadcast_to %150, %151 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %153 = torch.aten.sub.Tensor %147, %152, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %154 = torch.aten.mul.Tensor %153, %153 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %155 = torch.aten.sum.dim_IntList %154, %148, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %156 = torch.aten.div.Scalar %155, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %157 = torch.aten.add.Scalar %156, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %158 = torch.aten.rsqrt %157 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %159 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %160 = torch.aten.broadcast_to %158, %159 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %161 = torch.aten.mul.Tensor %153, %160 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %162 = torch.aten.mul.Tensor %161, %9 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %163 = torch.aten.add.Tensor %162, %8, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %164 = torch.aten.transpose.int %7, %int0, %int1 : !torch.vtensor<[2048,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,2048],f32>
  %165 = torch.aten.matmul %163, %164 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %166 = torch.aten.add.Tensor %165, %6, %int1 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048],f32>, !torch.int -> !torch.vtensor<[10,32,2048],f32>
  %167 = torch.aten.relu %166 : !torch.vtensor<[10,32,2048],f32> -> !torch.vtensor<[10,32,2048],f32>
  %168 = torch.aten.transpose.int %5, %int0, %int1 : !torch.vtensor<[256,2048],f32>, !torch.int, !torch.int -> !torch.vtensor<[2048,256],f32>
  %169 = torch.aten.matmul %167, %168 : !torch.vtensor<[10,32,2048],f32>, !torch.vtensor<[2048,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %170 = torch.aten.add.Tensor %169, %4, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %171 = torch.aten.add.Tensor %163, %170, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %172 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
  %173 = torch.aten.sum.dim_IntList %171, %172, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %174 = torch.aten.div.Scalar %173, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %175 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %176 = torch.aten.broadcast_to %174, %175 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %177 = torch.aten.sub.Tensor %171, %176, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %178 = torch.aten.mul.Tensor %177, %177 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %179 = torch.aten.sum.dim_IntList %178, %172, %true, %none : !torch.vtensor<[10,32,256],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[10,32,1],f32>
  %180 = torch.aten.div.Scalar %179, %int256 : !torch.vtensor<[10,32,1],f32>, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %181 = torch.aten.add.Scalar %180, %float1.000000e-05, %int1 : !torch.vtensor<[10,32,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[10,32,1],f32>
  %182 = torch.aten.rsqrt %181 : !torch.vtensor<[10,32,1],f32> -> !torch.vtensor<[10,32,1],f32>
  %183 = torch.prim.ListConstruct %int10, %int32, %int256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
  %184 = torch.aten.broadcast_to %182, %183 : !torch.vtensor<[10,32,1],f32>, !torch.list<int> -> !torch.vtensor<[10,32,256],f32>
  %185 = torch.aten.mul.Tensor %177, %184 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[10,32,256],f32> -> !torch.vtensor<[10,32,256],f32>
  %186 = torch.aten.mul.Tensor %185, %3 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32> -> !torch.vtensor<[10,32,256],f32>
  %187 = torch.aten.add.Tensor %186, %2, %int1 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256],f32>, !torch.int -> !torch.vtensor<[10,32,256],f32>
  %188 = torch.aten.transpose.int %1, %int0, %int1 : !torch.vtensor<[1000,256],f32>, !torch.int, !torch.int -> !torch.vtensor<[256,1000],f32>
  %189 = torch.aten.matmul %187, %188 : !torch.vtensor<[10,32,256],f32>, !torch.vtensor<[256,1000],f32> -> !torch.vtensor<[10,32,1000],f32>
  %190 = torch.aten.add.Tensor %189, %0, %int1 : !torch.vtensor<[10,32,1000],f32>, !torch.vtensor<[1000],f32>, !torch.int -> !torch.vtensor<[10,32,1000],f32>
  return %190 : !torch.vtensor<[10,32,1000],f32>
}